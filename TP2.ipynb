{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : TEXT PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUKDACH Yassine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to /home/yassine/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yassine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token = []\n",
    "StopWords = []\n",
    "Token_Maj = []\n",
    "for name, document in enumerate(field):\n",
    "    word_token = word_tokenize(webtext.raw(document))\n",
    "    nbr_stpwords = len([w for w in word_token if w in stop_words])\n",
    "    nbr_token = len(word_token)\n",
    "    nbr_token_M = len([w for w in word_token if w.isupper()])\n",
    "    StopWords.append(nbr_stpwords)\n",
    "    Token.append(nbr_token)\n",
    "    Token_Maj.append(nbr_token_M)\n",
    "#print(Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'Name_doc': webtext.fileids(),\n",
    "    'Token': Token,\n",
    "    'Token_Maj': Token_Maj,\n",
    "    'StopWords': StopWords\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name_doc</th>\n",
       "      <th>Token</th>\n",
       "      <th>Token_Maj</th>\n",
       "      <th>StopWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>firefox.txt</td>\n",
       "      <td>96079</td>\n",
       "      <td>2079</td>\n",
       "      <td>25053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grail.txt</td>\n",
       "      <td>16457</td>\n",
       "      <td>1760</td>\n",
       "      <td>3308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overheard.txt</td>\n",
       "      <td>210091</td>\n",
       "      <td>7457</td>\n",
       "      <td>55597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pirates.txt</td>\n",
       "      <td>21778</td>\n",
       "      <td>1771</td>\n",
       "      <td>5790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>singles.txt</td>\n",
       "      <td>4465</td>\n",
       "      <td>410</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wine.txt</td>\n",
       "      <td>31141</td>\n",
       "      <td>676</td>\n",
       "      <td>7901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name_doc   Token  Token_Maj  StopWords\n",
       "0    firefox.txt   96079       2079      25053\n",
       "1      grail.txt   16457       1760       3308\n",
       "2  overheard.txt  210091       7457      55597\n",
       "3    pirates.txt   21778       1771       5790\n",
       "4    singles.txt    4465        410        638\n",
       "5       wine.txt   31141        676       7901"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data_dict)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmove all stopwords\n",
    "def clean_stopwords(text):\n",
    "    clean_stopwords = [item for item in text if item not in stop_words]\n",
    "    return clean_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all characters into lowercase\n",
    "def convert_to_lower(text):\n",
    "    lower_tokens = [token.lower() for token in text]\n",
    "    return  lower_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', str(sen))\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' lovely delicate fragrant rhone wine polished leather and strawberries perhaps bit dilute but good for drinking now liquorice cherry fruit simple and coarse at the finish thin and completely uninspiring rough no stars big fat textured chardonnay nuts and butterscotch slightly odd metallic cardboard finish but probably blind tasting other than the fizz which included five vintages of cote rotie brune et blonde from guigal surprisingly young feeling and drinking well but without any great complexi'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = preprocess_text(word_token)\n",
    "clean_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print les mots les plus frÃ©quents ()\n",
    "for name, document in enumerate(field):\n",
    "    toknes = word_tokenize(webtext.raw(document))\n",
    "    no_stop_words = clean_stopwords(toknes)\n",
    "    to_lower = convert_to_lower(no_stop_words)\n",
    "    text_doc = preprocess_text(to_lower)\n",
    "    words = word_tokenize(text_doc)\n",
    "#words[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 363),\n",
       " ('quite', 303),\n",
       " ('fruit', 300),\n",
       " ('wine', 234),\n",
       " ('bit', 217),\n",
       " ('top', 216),\n",
       " ('very', 211),\n",
       " ('lovely', 163),\n",
       " ('touch', 160),\n",
       " ('dry', 159)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the 10 most frequent words from data \n",
    "from collections import Counter\n",
    "most_occ = Counter(words)\n",
    "most_occ.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spot',\n",
       " 'variety',\n",
       " 'characteristically',\n",
       " 'overawed',\n",
       " 'rayas',\n",
       " 'ideally',\n",
       " 'dies',\n",
       " 'average',\n",
       " 'set',\n",
       " 'ul']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "#Rare word removal\n",
    "def rarewords(token):\n",
    "    # FreqDist() function to get the distribution of the terms in the corpus,\n",
    "    freq_dist = nltk.FreqDist(token)\n",
    "    rarewords = list(freq_dist.keys())[-10:]\n",
    "    return rarewords\n",
    "rarewords(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words \n",
    "def num_to_words(text):\n",
    "\n",
    "    after_spliting = word_tokenize(text)\n",
    "\n",
    "    for index in range(len(after_spliting)):\n",
    "        if after_spliting[index].isdigit():\n",
    "            after_spliting[index] = num2words(after_spliting[index])\n",
    "\n",
    "    # joining list into string with space\n",
    "    numbers_to_words = ' '.join(after_spliting)\n",
    "    return numbers_to_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi , my name is chatboot , i have two years old , i was born at two thousand and eighteen'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Hi, my name is chatboot, i have 2 years old, i was born at 2018'\n",
    "num_to_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\"  # emoticons\n",
    "                               , flags=re.UNICODE)\n",
    "\n",
    "    without_emoji = emoji_pattern.sub(r'',text)\n",
    "    return without_emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by github\n",
    "https://gist.github.com/saimadhu-polamuri/d55b442e3166bfdab02e43407c38aa39#file-dataaspirant-text-preprocessing-techniques-removing-emojis-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is a test   \\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_emoji = \"\"\"\n",
    "This is a test ðŸ˜» ðŸ™ƒ ðŸ¤“\n",
    "\"\"\"\n",
    "remove_emojis(ex_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'delic', ',', 'fragrant', 'rhone', 'wine', '.', 'polish', 'leather', 'and', 'strawberri', '.', 'perhap', 'a', 'bit', 'dilut', ',', 'but', 'good', 'for', 'drink', 'now', '.', '***', 'liquoric', ',', 'cherri', 'fruit', '.', 'simpl', 'and', 'coars', 'at', 'the', 'finish', '.', '**', 'thin', 'and', 'complet', 'uninspir', '.', '*', 'rough', '.', 'No', 'star', 'big', ',', 'fat', ',', 'textur', 'chardonnay', '-', 'nut', 'and', 'butterscotch', '.', 'A', 'slightli', 'odd', 'metallic/cardboard', 'finish', ',', 'but', 'probabl', '***', 'A', 'blind', 'tast', ',', 'other', 'than', 'the', 'fizz', ',', 'which', 'includ', 'five', 'vintag', 'of', 'cote', 'roti', 'brune', 'et', 'blond', 'from', 'guigal', '.', 'surprisingli', 'young', 'feel', 'and', 'drink', 'well', ',', 'but', 'without', 'ani', 'great']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in word_token]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancaster Stemmer\n",
      "['lov', 'del', ',', 'fragr', 'rhon', 'win', '.', 'pol', 'leath', 'and', 'strawberries', '.', 'perhap', 'a', 'bit', 'dilut', ',', 'but', 'good', 'for', 'drink', 'now', '.', '***', 'liqu', ',', 'cherry', 'fruit', '.', 'simpl', 'and', 'coars', 'at', 'the', 'fin', '.', '**', 'thin', 'and', 'complet', 'uninspir', '.', '*', 'rough', '.', 'no', 'star', 'big', ',', 'fat', ',', 'text', 'chardonnay', '-', 'nut', 'and', 'butterscotch', '.', 'a', 'slight', 'od', 'metallic/cardboard', 'fin', ',', 'but', 'prob', '***', 'a', 'blind', 'tast', ',', 'oth', 'than', 'the', 'fizz', ',', 'which', 'includ', 'fiv', 'vint', 'of', 'cot', 'roty', 'brun', 'et', 'blond', 'from', 'guig', '.', 'surpr', 'young', 'feel', 'and', 'drink', 'wel', ',', 'but', 'without', 'any', 'gre']\n"
     ]
    }
   ],
   "source": [
    "lancaster=LancasterStemmer()\n",
    "print(\"Lancaster Stemmer\")\n",
    "stemmed = [lancaster.stem(word) for word in word_token]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PorterStemmer: nous allons uniquement garder la racin du mot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_word = [lemma.lemmatize(word) for word in word_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lovely', 'delicate', ',', 'fragrant', 'Rhone', 'wine', '.', 'Polished', 'leather', 'and', 'strawberry', '.', 'Perhaps', 'a', 'bit', 'dilute', ',', 'but', 'good', 'for', 'drinking', 'now', '.', '***', 'Liquorice', ',', 'cherry', 'fruit', '.', 'Simple', 'and', 'coarse', 'at', 'the', 'finish', '.', '**', 'Thin', 'and', 'completely', 'uninspiring', '.', '*', 'Rough', '.', 'No', 'Stars', 'Big', ',', 'fat', ',', 'textured', 'Chardonnay', '-', 'nut', 'and', 'butterscotch', '.', 'A', 'slightly', 'odd', 'metallic/cardboard', 'finish', ',', 'but', 'probably', '***', 'A', 'blind', 'tasting', ',', 'other', 'than', 'the', 'fizz', ',', 'which', 'included', 'five', 'vintage', 'of', 'Cote', 'Rotie', 'Brune', 'et', 'Blonde', 'from', 'Guigal', '.', 'Surprisingly', 'young', 'feeling', 'and', 'drinking', 'well', ',', 'but', 'without', 'any', 'great']\n"
     ]
    }
   ],
   "source": [
    "print(lemma_word[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet :on va  va laisser au mot un sens sÃ©mantique mais on va Ã©liminer le genre ou le pluriel par exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
